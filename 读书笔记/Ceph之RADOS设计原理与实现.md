# Ceph之RADOS设计原理与实现

## RADOS导论

- Ceph三大核心应用

![image-20221109201741961](https://s2.loli.net/2022/11/09/qBec5iaH9lV2Uvm.png)

---

### 1.1 RADOS概述

​	一个RADOS集群由大量OSD（Object Store Device，对象存储设备）和少数几个Monitor组成。

​	OSD是个抽象概念，一般对应一个本地块设备（如一块磁盘或者一个RAID组等），在其工作周期内会占用一些CPU、内存、网络等物理资源，并依赖某个具体的本地对象存储引擎，来访问位于块设备上的数据。

​	基于高可靠设计的Monitor团体（quorum，本质上也是一个集群）则负责维护和分发集群的关键元数据，同时也是客户端与RADOS集群建立连接的桥梁——客户端通过咨询Monitor获得集群的关键元数据之后，就可以通过约定的方式（例如RBD、RGW、CephFS等）来访问RADOS集群.

![image-20221109202540260](https://s2.loli.net/2022/11/09/ERF3UdOvqZH4iQx.png)

​	为了去中心化、免除单点故障，RADOS使用一张紧凑的集群表对集群拓扑结构和数据映射规则进行描述。任何时刻，任何持有该表的合法客户端都可以独立地与位于任意位置的OSD直接通信。当集群拓扑结构发生变化时，RADOS确保这些变化能够及时地被Monitor捕获，并通过集群表高效传递至所有受影响的OSD和客户端，以保证对外提供不中断的业务访问。由于数据复制、故障检测和数据恢复都由每个OSD自动进行，因此即便存储容量上升至PB级别或者以上，系统也不会存在明显的调度和处理瓶颈。

---

### 1.2 存储池和PG

​	为了实现存储资源按需配置，RADOS对集群中的OSD进行池化管理。资源池（对存储系统而言，具体则是存储池）实际上是个虚拟概念，表示一组约束条件，例如可以针对存储池设计一组CRUSH规则，限制其只能使用某些规格相同的OSD，或者尽量将所有数据副本分布在物理上隔离的、不同的故障域；

为了实现不同存储池之间的策略隔离，RADOS并不是将任何应用程序数据一步到位地写入OSD的本地存储设备，而是引入了一个中间结构，称为PG（Placement Group），执行两次映射。
	**第一次映射是静态的，负责将任意类型的客户端数据按照固定大小进行切割、编号后，作为伪随机哈希函数输入，均匀映射至每个PG，以实现负载均衡策略；第二次映射实现PG到OSD的映射，仍然采用伪随机哈希函数（以保证PG在OSD之间分布的均匀性），但是其输入除了全局唯一的PG身份标识之外，还引入了集群拓扑，并且使用CRUSH规则对映射过程进行调整，以帮助PG在不同OSD之间灵活迁移，进而实现数据可靠性、自动平衡等高级特性。最终，存储池以PG作为基本单位进行管理。**

![image-20221109204459894](https://s2.loli.net/2022/11/09/cO5miwFY1LvVNZ7.png)

​	为了维持扁平寻址空间，实际上要求PG拥有一个全集群唯一的身份标识——PGID。由于集群所有存储池都由Monitor统一管理，所以Monitor可以为每个存储池分配一个集群内唯一的存储池标识。基于此，我们只需要为存储池中的每个PG再分配一个存储池内唯一的编号即可。假定某个存储池的标识为1，创建此存储池时指定了256个PG，那么容易理解对应的PGID可以写成1.0，1.1，…，1.255这样的形式。

---

### 1.3 stable_mod和客户端寻址

​	Ceph通过C/S模式实现外部应用程序与RADOS集群之间的交互。任何时候，应用程序通过客户端访问集群时，首先由其客户端负责生成一个字符串形式的对象名，然后基于对象名计算得出一个32位哈希值。针对此哈希值，通过简单的数学运算，例如对存储池的PG数（pg_num）取模，可以得到一个PG在存储池内部的编号，加上对象本身已经记录了其归属存储池的唯一标识，最终可以找到负责承载该对象的PG。


$$
pg\_num > 2^n
$$


> 假定标识为1的存储池中的某个对象，经过计算后32位哈希值为0x4979FA12，并且该存储池的pg_num为256，则由：
> 0x4979FA12 mod 256 = 18
> 我们知道此对象由存储池内编号为18的PG负责承载，并且其完整的PGID为1.18。
>
> 一般而言，将某个对象映射至PG时，我们并不会使用全部的32位哈希值，因此会出现不同对象被映射至同一个PG的现象。我们很容易验证该存储池内哈希值如下的其他对象，通过模运算同样会被映射至PGID为1.18的PG之上。
> 0x4979FB12 mod 256 = 18
> 0x4979FC12 mod 256 = 18
> 0x4979FD12 mod 256 = 18
> …
>
> 可见，针对上面这个例子，我们仅仅使用了这个“全精度”32位哈希值的后8位。因此，如果pg_num可以写成2n的形式（例如这里256可以写成28，即是2的整数次幂），则每个对象执行PG映射时，其32位哈希值中仅有低n位是有意义的。进一步地，我们很容易验证此时归属于同一个PG的对象，其32位哈希值中低n位都是相同的。基于此，我们将2n-1称为pg_num的掩码，其中n为掩码的位数。

$$
2^n>pg\_num>2^{n-1}
$$



> 相反，如果pg_num不是2的整数次幂，即无法写成2n的形式，仍然假定此时其最高位为n（从1开始计数），则此时普通的取模操作无法保证“归属于某个PG的所有对象的低n位都相同”这个特性。例如，假定pg_num为12，此时n=4，容易验证如下输入经过模运算后结果一致，但是它们只有低2位是相同的。
> 0x00 mod 12 = 0
> 0x0C mod 12 = 0
> 0x18 mod 12 = 0
> 0x24 mod 12 = 0
> …
> 因此需要针对普通的取模操作加以改进，以保证针对不同输入，当计算结果相等时，维持“输入具有尽可能多的、相同的低位”这个相对有规律的特性。
>
> 一种改进的方案是使用掩码来替代取模操作。例如仍然假定pg_num对应的最高位为n，则其掩码为2n-1，需要将某个对象映射至PG时，直接执行hash &(2n-1)即可。但是这个改进方案仍然存在一个潜在问题：如果pg_num不是2的整数次幂，那么直接采用这种方式进行映射会产生空穴，即将某些对象映射到一些实际上并不存在的PG上。如图所示：这里pg_num为12，n=4，可见执行hash &(24-1)会产生0～15共计16种不同的结果。但是实际上编号为12～15的PG并不存在。
>
> ![image-20221110141524861](https://s2.loli.net/2022/11/10/TuOCxogtPmEIedN.png)
>
> 
>
> 因此需要进一步对上述改进方案进行修正。由n为pg_num的最高位，必然有：
> pg_num ≥ 2n-1
> 即[0，2n-1]内的PG必然都是存在的。于是可以通过hash &(2n-1-1)将那些实际上并不存在的PG重新映射到[0，2n-1]区间，如图所示:
>
> ![image-20221110142119958](https://s2.loli.net/2022/11/10/i39Yk6yAvJgTL8q.png)
>
> 修正后的效果等同于将这些空穴通过平移的方式重定向到前半个对称区间。

- **<font color="red">stable_num算法</font>**

![image-20221110144148806](https://s2.loli.net/2022/11/10/Uc7MCf6bxyVaQpS.png)

​	综上，无论pg_num是否为2的整数次幂，采用stable_mod都可以产生一个相对有规律的对象到PG的映射结果，这是PG分裂的一个重要理论基础。

---

### 1.4 PG分裂与集群扩容

> 如果分裂为4096个PG（此时n=12，对应的掩码为212-1=4095），则原来每个PG对应分裂成16个PG。仍以PGID为Y.D2的PG为例，通过简单计算可以得到这15个新增PG的PGID分别为：
> Y.1D2
> Y.2D2
> …
> Y.ED2
> Y.FD2
> 可见这些新的PG对应的对象分别存储于老PG的如下目录之下。
> ./2/D/1/
> ./2/D/2/
> …
> ./2/D/E/
> ./2/D/F/
> 此时可以不用移动对象（即文件），而是直接修改文件夹的归属，即可完成对象在新老PG之间的迁移。

## CRUSH算法

### 2.1 抽签算法

​	Ceph在设计之初被定位成用于管理大型分级存储网络的分布式文件系统。网络中的不同层级具有不同的故障容忍程度，因此也称为故障域。如图所示,展示了一个具有3个层级（机架、主机、磁盘）的Ceph集群。

![image-20221110163204466](https://s2.loli.net/2022/11/10/WhuoPVcO3HvU7Ak.png)
$$
图2-1 具有3个层级的Ceph集群
$$
​	在图2-1中，单个主机包含多个磁盘，每个机架包含多个主机，并采用独立的供电和网络交换系统，从而可以将整个集群以机架为单位划分为若干故障域。**为了实现高可靠性，实际上要求数据的多个副本分布在不同机架的主机磁盘之上。因此，CRUSH首先应该是一种基于层级的深度优先遍历算法。此外，上述层级结构中，每个层级的结构特征也存在差异，一般而言，越处于顶层的其结构变化的可能性越小，反之越处于底层的则其结构变化越频繁。**例如，大多数情况下一个Ceph集群自始至终只对应一个数据中心，但是主机或者磁盘数量随时间流逝则可能一直处于变化之中。**因此，从这个角度而言，CRUSH还应该允许针对不同的层级按照其特点设置不同的选择算法，从而实现全局和动态最优**。

​	Sage一共设计了4种不同的基本选择算法，这些算法是实现其他更复杂算法的基础，它们各自的优缺点如表2-1所示。
$$
表2-1 CRUSH基本选择算法
$$

|            | unique | list |   tree    | straw |
| :--------: | :----: | :--: | :-------: | :---: |
| 时间复杂度 |  O(1)  | O(N) | O(log(N)) | O(N)  |
|  添加元素  |   差   | 最好 |    好     | 最好  |
|  删除元素  |   差   |  差  |    好     | 最好  |

> ​	由表2-1可见,unique算法执行效率最高，但是抵御结构变化的能力最差；straw算法执行效率较低，但是抵御结构变化的能力最好；list和tree算法执行效率和抵御结构变化的能力介于unique与straw之间。如果综合考虑呈爆炸式增长的存储空间需求（导致需要添加元素）、在大型分布式存储系统中某些部件故障是常态（导致需要删除元素），以及愈发严苛的数据可靠性需求（导致需要将数据副本存储在更高级别的故障域中，例如不同的数据中心），那么针对任何层级采用straw算法都是一个不错的选择。事实上，这也是CRUSH算法的现状，在大多数将Ceph用于生产环境的案例中，除了straw算法之外，其他3种算法基本上形同虚设，因此我们将重点放在分析straw算法上。

​	顾名思义，**straw算法将所有元素比作吸管，针对指定输入，为每个元素随机地计算一个长度，最后从中选择长度最长的那个元素（吸管）作为输出。这个过程被形象地称为抽签（draw），元素的长度称为签长。**

​	显**然straw算法的关键在于如何计算签长。理论上，如果所有元素构成完全一致，那么只需要将指定输入和元素自身唯一编号作为哈希输入即可计算出对应元素的签长。因此，如果样本容量足够大，那么最终所有元素被选择的概率都是相等的，从而保证数据在不同元素之间均匀分布。**然而实际上，前期规划得再好的集群，其包含的存储设备随着时间的推移也会逐渐趋于异构化，例如，由于批次不同而导致的磁盘容量差异。

​	**显然，通常情况下，我们不应该对所有设备一视同仁，而是需要在CRUSH算法中引入一个额外的参数，称为权重，来体现设备之间的差异，让权重大（对应容量大）的设备分担更多的数据，权重小（对应容量小）的设备分担更少的数据，从而使得数据在异构存储网络中也能合理地分布。**

​	**将上述理论应用于straw算法，则可以通过使用权重对签长的计算过程进行调整来实现，即我们总是倾向于让权重大的元素有较大的概率获得更大的签长，从而在每次抽签中更容易胜出**。因此，引入权重之后straw算法的执行结果将取决于3个因素：固定输入、元素编号和元素权重。这其中，元素编号起的是随机种子的作用，所以针对固定输入，straw算法实际上只受元素权重的影响。进一步地，如果每个元素的签长只与自身权重相关，则可以证明此时straw算法对于添加元素和删除元素的处理都是最优的。我们以添加元素为例进行论证。

> 1）假定当前集合中一共包含n个元素：
> (e1，e2，…，en)
> 2）向集合中添加新元素en+1：
> (e1，e2，…，en，en+1)
> 3）针对任意输入x，加入en+1之前，分别计算每个元素签长并假定其中最大值为dmax：
> (d1，d2，…，dn)
> 4）因为新元素en+1的签长计算只与自身编号及自身权重相关，所以可以使用x独立计算其签长（同时其他元素的签长不受en+1加入的影响），假定为dn+1；
> 5）又因为straw算法总是选择最大的签长作为最终结果，所以：
> 如果dn+1>dmax，那么x将被重新映射至新元素en+1；反之，对x的已有映射结果无任何影响。
>
> ​	可见，添加一个元素，straw算法会随机地将一些原有元素中的数据重新映射至新加入的元素之中。同理，删除一个元素，straw算法会将该元素中全部数据随机地重新映射至其他元素之中。因此无论添加或者删除元素，都不会导致数据在除被添加或者删除之外的两个元素（即不相干的元素）之间进行迁移。

***

### 2.2 CRUSH算法详解

​	CRUSH基于上述基本选择算法完成数据映射，这个过程是受控的并且高度依赖于集群的拓扑描述——cluster map。不同的数据分布策略通过制定不同的placement rule来实现。placement rule实际上是一组包括最大副本数、故障（容灾）级别等在内的自定义约束条件，例如针对图2-1所示的集群，我们可以通过一条placement rule将互为镜像的3个数